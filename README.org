#+TITLE: Report oft Practical Course on High-Performance Computing
#+SUBTITLE: 
#+SUBTITLE: Parallel Deep Learning pipelines using Go and MPI
#+SUBTITLE:
#+AUTHOR: Persentor: Silin Zhao 
#+AUTHOR: Supervisor: Patrick Michaelis
#+OPTIONS: toc:nil
#+OPTIONS: num:t
#+STARTUP: beamer
#+DATA: September 13 2022
* Project notation
** Youtube link
[[https://www.youtube.com/watch?v=2siZQBvRPuY&t=6s]]
** Datasets

- This project source code can be found [[https://github.com/scofild429/go_mpi_network]],This is the *README* page.
- Iris dataset (https://www.kaggle.com/datasets/saurabh00007/iriscsv)
- Intel image classification, (https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download). Download it,  put archive it in the folder ./datasets/

*All training data will equally divied for each training network, specially for mpi*

* Configuration  example
+ ./goai/.irisenv
+ ./goai/.imgenv
#+begin_src
    inputdataDims=4
    inputLayerNeurons=30
    hiddenLayerNeurons=20
    outputLayerNeurons=3
    labelOnehotDims=3
    numEpochs=100
    learningRate=0.01
    batchSize=4
#+end_src

* Sumbit the job in cluster

*no singularity*, installing golang 1.18 was failed always

using binary executable code of golang, *go build*

#+begin_src sh
  #!/bin/bash
  #SBATCH --job-name mpi-go-neural-network
  #SBATCH -N 1
  #SBATCH -p fat
  #SBATCH -n 20
  #SBATCH --time=01:30:00

  module purge
  module load openmpi

  mpirun -n 20 ./goai
#+end_src

* Deep learning's problem

As AI comes to deep learning, the computing resource becomes more critical for training process.

*Applications:*
+ Image Classification
+ NLP
+ Semantic segmentation

*Solution*
+ GPU
+ TPU
+ *Distributed learning*

* Single network architecture
#+begin_src 
 raining data -> inputLayer(w1, b1) -> dinputLayer
 Normalization
 dinputLayer -> hiddenLayer(w2, b2) -> dhiddenLayer
 Normalization
 dhiddenLayer -> OutputLayer(w3, b3) -> doutputLayer
 #+end_src
 Loss = L2: (doutputLayer - onehotlable)^2
#+begin_src  
 Backpropagation from Loss  of Outputlayer  to w3, b3
 Backpropagation from error of Hiddenlayer  to w2, b2
 Backpropagation from error of Inputlayer   to w1, b1
 #+end_src
 
 Derivative of sigmoid, Normalization, Standardization
 
 - Stochastic Gradient Descent (SGD)
 - Mini-batch Gradient Descent (MBGD)
 - Batch Gradient Descent (BGD)

* Illustration of weights updating
#+ATTR_LATEX: :width 0.8\textwidth
[[./png/NeuralNetwork.png]]

* Code implementation
#+begin_src go :exports both :results output
  func main() {
          singlenode.Single_node_iris(true)
          mpicode.Mpi_iris_Allreduce()
          mpicode.Mpi_iris_SendRecv()
          mpicode.Mpi_images_Allreduce()
          mpicode.Mpi_images_SendRecv()
  }
#+end_src

You can review my code, and choose one of them to be executed in /goai/myai.go main function.

Comparing with python:

+ ./pytorchDemo/irisfromscratch.py
+ ./pytorchDemo/iriswithpytorch.py
+ ./pytorchDemo/logisticRcuda.py

* Network performance(iris dataset)
   :PROPERTIES:
   :BEAMER_envargs: [t]
   :END:
** Loss
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :END:
[[./png/single_node_loss.png]]

** Accuarcy
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :BEAMER_envargs: <2->
    :END:
[[./png/single_node_acc.png]]

* MPI communication
#+begin_src
github.com/sbromberger/gompi
import CGO as C
#+end_src

 + *Collective*
   - gompi.BcastFloat64s() -> C.MPI \textunderscore Bcast()
   - gompi.AllreduceFloat64s -> C.MPI \textunderscore Allreduce()
   
 + *Non Collective*
   - gompi.SendFloat64s() -> C.MPI \textunderscore Send()
   - gompi.SendFloat64() -> C.MPI \textunderscore Send()
   - gompi.RecvFloat64s() -> C.MPI \textunderscore Recv()
   - gompi.RecvFloat64() -> C.MPI \textunderscore Recv()

* Non collective architecture
#+ATTR_LATEX: :width 0.8\textwidth
[[./png/MPINetworkSendRecv.png]]

* Non collective design
** rank = 0
+ in *main network* weights will be initialized, but not for training,
+ weights will broadcast to all other training networks
** rank != 0
+ in *train network* receive weights from main network for initialization
+ After each batch training done, sending its weights variance to main network
** rank = 0
+ receiving the  variance from all training network
+ accumulating and then sending back to training network
** rank != 0
+ start next training batch

* Collective architecture
#+ATTR_LATEX: :width 0.8\textwidth
[[./png/MPINetworkAllreduce.png]]
* Collective design
+ All network train its data respectively,
+ After each train batch, pack all weights into array
+ MPI_Allreduce for new array
+ updating weights with  new array

* Iris dataset performance for non-collective
   :PROPERTIES:
   :BEAMER_envargs: [t]
   :END:
** Send&Recv loss
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :END:
[[./png/iris_sendrecv_loss.png]]

** Send&Recv accuracy
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :BEAMER_envargs: <2->
    :END:
[[./png/iris_sendrecv_accuracy.png]]

* Iris dataset performance for collective
   :PROPERTIES:
   :BEAMER_envargs: [t]
   :END:
** Allreduce loss
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :END:
[[./png/iris_allreduce_loss.png]]
** Allreduce accuracy
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :BEAMER_envargs: <2->
    :END:
[[./png/iris_allreduce_accuracy.png]]

* Intel image classification performance
   :PROPERTIES:
   :BEAMER_envargs: [t]
   :END:
** Send&Recv loss (220 images)
    :PROPERTIES:
    :BEAMER_col: 0.55
    :BEAMER_env: block
    :END:
[[./png/intelImage_subset_sendrecving_loss.png]]
 SendRecv loss (14000 images)
[[./png/intelImage_sendrecv_loss.png]]
** Allreduce loss (220 images)
    :PROPERTIES:
    :BEAMER_col: 0.55
    :BEAMER_env: block
    :BEAMER_envargs: <2->
    :END:
[[./png/intelImage_subset_allreduce_loss.png]]
Allreduce loss (14000 images)
[[./png/intelImage_allreduce_loss.png]]
* Speedup Diagrams
   :PROPERTIES:
   :BEAMER_envargs: [t]
   :END:
** Iris for Allreduce and Send&Recv with different nodes
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :END:
[[./png/irisSpendup.png]]
** Intel Image Classification for Allreduce and Send&Recv with different nodes
    :PROPERTIES:
    :BEAMER_col: 0.6
    :BEAMER_env: block
    :BEAMER_envargs: <2->
    :END:
[[./png/intelImageSpendup.png]]
* Discussion

*neural network model implement is not perfect, so the accuracy performance not so well*

*For each epoch:*
+ Allreduce: about 2 minutes
+ Send&Recv: about 3.6 minutes, because of synchronization of each batch training


*Change nodes, scaling behavior, such as speedup diagrams is missing*

*Change the batchsize, reducing mpi communication*

* Conclusion
+ Golang can also be used for parallel computing
+ neural network implementation of golang can be improved
+ HPC cluster for distributed learning has significant benefits for large dataset




