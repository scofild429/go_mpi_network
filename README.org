#+TITLE: Report oft Practical Course on High-Performance Computing
#+SUBTITLE: 
#+SUBTITLE: Parallel Deep Learning pipelines using Go and MPI
#+SUBTITLE: 
#+AUTHOR: Persentor: Silin Zhao 
#+AUTHOR: Supervisor: Patrick Michaelis
#+OPTIONS: num:t
#+STARTUP: overview
#+DATA: September 13 2022
* Project notation
** Datasets
This project source code can be found [[https://github.com/scofild429/go_mpi_network]],
This is the *README* page.

Iris dataset (https://www.kaggle.com/datasets/saurabh00007/iriscsv)

Intel image classification
(https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download).
Download it,  put archive it in the folder ./datasets/

*All training data will equally divied for each training network, specially for mpi*

** Configuration 
+ ./goai/.irisenv
+ ./goai/.imgenv
#+begin_src
    inputdataDims=4
    inputLayerNeurons=30
    hiddenLayerNeurons=20
    outputLayerNeurons=3
    labelOnehotDims=3
    numEpochs=100
    learningRate=0.01
    batchSize=4
#+end_src

** Sumbit the job in cluster

*no singularity*, installing golang 1.18 was failed always

using binary executable code of golang, *go build*

#+begin_src sh
  #!/bin/bash
  #SBATCH --job-name mpi-go-neural-network
  #SBATCH -N 1
  #SBATCH -p fat
  #SBATCH -n 20
  #SBATCH --time=01:30:00

  module purge
  module load openmpi

  mpirun -n 20 ./goai
#+end_src

* Deep learning's problem

As AI comes to deep learning, the computing resource becomes more critical for training process.

*Applications:*
+ Image Classification
+ NLP
+ Semantic segmentation

*Solution*
+ GPU
+ TPU
+ *Distributed learning*

* Single network architecture
 raining data -> inputLayer(w1, b1, sigmoid) -> dinputLayer
 Normalization
 
 dinputLayer -> hiddenLayer(w2, b2, sigmoid) -> dhiddenLayer
 Normalization
 
 dhiddenLayer -> OutputLayer(w3, b3, sigmoid) -> doutputLayer
 
 Loss = L2: (doutputLayer - onehot_lable)^2
 
 Backpropagation from Loss  of Outputlayer  to w3, b3
 Backpropagation from error of Hiddenlayer  to w2, b2
 Backpropagation from error of Inputlayer   to w1, b1

 Derivative of sigmoid, Normalization, Standardization
 
 - Stochastic Gradient Descent (SGD)
 - Mini-batch Gradient Descent (MBGD)
 - Batch Gradient Descent (BGD)

* Illustration of weights updating
[[./png/NeuralNetwork.png]]

* Code implementation
#+begin_src go
    func main() {
          singlenode.Single_node_iris(true)
          // mpicode.Mpi_iris_Allreduce()
          // mpicode.Mpi_iris_SendRecv()
          // mpicode.Mpi_images_Allreduce()
          // mpicode.Mpi_images_SendRecv()
  }
#+end_src

You can review my code, and choose one of them to be trained in /goai/myai.go main function.

comparing with python:

+ ./pytorchDemo/irisfromscratch.py
+ ./pytorchDemo/iriswithpytorch.py
+ ./pytorchDemo/logisticRcuda.py

* Network performance(iris dataset)
** Loss
[[./png/single_node_loss.png]]
** Accuracy
[[./png/single_node_acc.png]]

* MPI communication

[[github.com/sbromberger/gompi][https://github.com/sbromberger/gompi]]
import CGO as C

 + *Collective*
   - gompi.BcastFloat64s() -> C.MPI_Bcast()
   - gompi.AllreduceFloat64s -> C.MPI_Allreduce()
   
 + *Non Collective*
   - gompi.SendFloat64s() -> C.MPI_Send()
   - gompi.SendFloat64() -> C.MPI_Send()
   - gompi.RecvFloat64s() -> C.MPI_Recv()
   - gompi.RecvFloat64() -> C.MPI_Recv()

* Non collective architecture
[[./png/MPINetworkSendRecv.png]]

* Non collective design
#+begin_src sh
  mpirun -n -20 ./goai
#+end_src
** rank = 0
+ *main network*
+ weights will be initialized, but not for training,
+ weights will broadcast to all other training networks
** rank != 0
+ *train network*
+ receive weights from main network for initialization
+ After each batch training done, sending its weights variance to main network
  
** rank = 0
+ receiving the  variance from all training network
+ and accumulate them
+ send back to training network
  
** rank != 0
+ start next training batch

* Collective architecture
[[./png/MPINetworkAllreduce.png]]

* Collective design
+ All network train its data respectively,
+ After each train batch, pack all weights into array
+ MPI_Allreduce for new array
+ updating weights with  new array

* Iris dataset performance 
** Send&Recv loss
[[./png/iris_sendrecv_loss.png]]

** Send&Recv accuracy
[[./png/iris_sendrecv_accuracy.png]]

** Allreduce loss
[[./png/iris_allreduce_loss.png]]

** Allreduce accuracy
[[./png/iris_allreduce_accuracy.png]]
* Intel image classification performance
** Send&Recv loss (220 images)

[[./png/intelImage_subset_sendrecving_loss.png]]
** Allreduce loss (220 images)

[[./png/intelImage_subset_allreduce_loss.png]]

** SendRecv loss (14000 images)

[[./png/intelImage_sendrecv_loss.png]]
** Allreduce loss (14000 images)

[[./png/intelImage_allreduce_loss.png]]

* Speedup Diagrams
** Speedup diagrams of iris for Allreduce and Send&Recv with different nodes
[[./png/irisSpendup.png]]


** Speedup diagrams of Intel Image Classification for Allreduce and Send&Recv with different nodes
[[./png/intelImageSpendup.png]]

* Discussion

*neural network model implement is not perfect, so the accuracy performance not so well*

*For each epoch:*
+ Allreduce: about 2 minutes
+ Send&Recv: about 3.6 minutes, because of synchronization of each batch training


*Change nodes, scaling behavior, such as speedup diagrams is missing*

*Change the batchsize, reducing mpi communication*

* Conclusion
+ Golang can also be used for parallel computing
+ neural network implementation of golang can be improved
+ HPC cluster for distributed learning has significant benefits for large dataset



