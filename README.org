#+TITLE: Report of Practical Course on High-Performance Computing
#+SUBTITLE: 
#+SUBTITLE: Parallel Deep Learning pipelines using Go and MPI
#+SUBTITLE: 
#+AUTHOR: Persentor: Silin Zhao 
#+AUTHOR: Supervisor: Patrick Michaelis
#+OPTIONS: num:t
#+STARTUP: overview
#+DATA: September 13 2022
* Project Notation
** Datasets
Project code can be found [[https://github.com/scofild429/go_mpi_network]],
This is the README page.

iris dataset (https://www.kaggle.com/datasets/saurabh00007/iriscsv)
Intel Image Classification
(https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download).
Download this dataset,  put archive it in the folder ./datasets/

training data will equally divied into each training networks

** Configuration
+ ./goai/.irisenv
+ ./goai/.imgenv
#+begin_src
    inputdataDims=4
    inputLayerNeurons=30
    hiddenLayerNeurons=20
    outputLayerNeurons=3
    labelOnehotDims=3
    numEpochs=100
    learningRate=0.01
    batchSize=4
    parallelism=8
#+end_src

** Sumbit the job in cluster
#+begin_src sh
  #!/bin/bash
  #SBATCH --job-name mpi-go-neural-network
  #SBATCH -N 1
  #SBATCH -p fat
  #SBATCH -n 20
  #SBATCH --time=01:30:00

  module purge
  module load openmpi

  mpirun -n 20 ./goai
#+end_src

* Deep Learning's problem
+ Image Classification
+ NLP
+ Sementic segmentation

As AI comes to deep learning, the computing resource becomes more critical for training process.

*Solution*
+ GPU
+ TPU
+ Distributed learning 

* Single network
 taining data -> inputLayer(w1, b1, sigmod) -> dinputLayer
 Normalization
 
 dinputLayer -> hiddenLayer(w2, b2, sigmod) -> dhiddenLayer
 Normalization
 
 dhiddenLayer -> OutputLayer(w3, b3, sigmod) -> doutputLayer
 
 Loss = L2: (doutputLayer - onehot_lable)^2
 
 Backpropagation from Loss  of Outputlayer  to w3, b3
 Backpropagation from error of Hiddenlayer  to w2, b2
 Backpropagation from error of Inputlayer   to w1, b1

 - Stochastic Gradient Descent (SGD)
 - Mini-batch Gradient Descent (MBGD)
 - Batch Gradient Descent (BGD)

* Illustration of weights updating
[[./NeuralNet.png]]

* Code implementation
#+begin_src go
    func main() {
          singlenode.Single_node_iris(true)
          // mpicode.Mpi_iris_Allreduce()
          // mpicode.Mpi_iris_SendRecv()
          // mpicode.Mpi_images_Allreduce()
          // mpicode.Mpi_images_SendRecv()
  }
#+end_src

You can review my code, and choose one of them to be trained in /goai/myai.go main function.

comparing with python:

+ ./pytorchDemo/irisfromscratch.py
+ ./pytorchDemo/iriswithpytorch.py
+ ./pytorchDemo/logisticRcuda.py

* MPI communication

[[github.com/sbromberger/gompi][https://github.com/sbromberger/gompi]]
import CGO as C

 + *Collective*
   gompi.BcastFloat64s() -> C.MPI_Bcast()
   gompi.AllreduceFloat64s -> C.MPI_Allreduce()
   
 + *Non Collective*
   gompi.SendFloat64s() -> C.MPI_Send()
   gompi.SendFloat64() -> C.MPI_Send()
   gompi.RecvFloat64s() -> C.MPI_Recv()
   gompi.RecvFloat64() -> C.MPI_Recv()

* Non collectives architecture
[[./MPINetArch.png]]

* Non collectives design
#+begin_src sh
  mpirun -n -20 ./goai
#+end_src
** rank = 0
+ *main network*
+ weights will be initialized, but not for training,
+ weights will broadcast to all other training networks
** rank != 0
+ *train netework*
+ receive weights from main network for initialization
+ After each batch training done, sending its weights variance to main network
  
** rank = 0
+ receiving the  variance from all training network
+ and accumulate them
+ send back to training network
  
** rank != 0
+ start next training batch

* Collective architecture
[[./MPINetArchAllreduce.png]]

* Collective design
+ All network train its data respectively,
+ After each train batch, pack all weights into array
+ MPI_Allreduce for new array
+ updating weights with  new array

* Project result
** single netework performance
*** Loss
*** Accuracy




** Non collective performance
** Collective performance

* Problems 
*model implement is not perfect**

* Conculution

